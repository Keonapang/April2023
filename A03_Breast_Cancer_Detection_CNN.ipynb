{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyHghTqk78biT55DaEfzJq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keonapang/April2023/blob/main/A03_Breast_Cancer_Detection_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIiPwFnUlYQe",
        "outputId": "fa63f907-ab6e-4ede-a21b-42245970301d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BDDw1RmjtVp"
      },
      "outputs": [],
      "source": [
        "# Important libraries (keras and scikit-learn)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten \n",
        "from keras.layers import Activation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data from the Breast Cancer Classification .csv file\n",
        "data_raw = pd.read_csv('/content/drive/MyDrive/Colab-Notebooks/BCW_dataset.csv', delimiter=',', header=0, index_col=None) # Head method show first 5 rows of data\n",
        "print(data_raw.head()) #prints only the first 5 rows for each 33 columns]"
      ],
      "metadata": {
        "id": "wu90SKQskDwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044f0335-cf21-4c13-a271-5bb9acc3c0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
            "0    842302         M        17.99         10.38          122.80     1001.0   \n",
            "1    842517         M        20.57         17.77          132.90     1326.0   \n",
            "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
            "3  84348301         M        11.42         20.38           77.58      386.1   \n",
            "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
            "\n",
            "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
            "0          0.11840           0.27760          0.3001              0.14710   \n",
            "1          0.08474           0.07864          0.0869              0.07017   \n",
            "2          0.10960           0.15990          0.1974              0.12790   \n",
            "3          0.14250           0.28390          0.2414              0.10520   \n",
            "4          0.10030           0.13280          0.1980              0.10430   \n",
            "\n",
            "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
            "0  ...          17.33           184.60      2019.0            0.1622   \n",
            "1  ...          23.41           158.80      1956.0            0.1238   \n",
            "2  ...          25.53           152.50      1709.0            0.1444   \n",
            "3  ...          26.50            98.87       567.7            0.2098   \n",
            "4  ...          16.67           152.20      1575.0            0.1374   \n",
            "\n",
            "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
            "0             0.6656           0.7119                0.2654          0.4601   \n",
            "1             0.1866           0.2416                0.1860          0.2750   \n",
            "2             0.4245           0.4504                0.2430          0.3613   \n",
            "3             0.8663           0.6869                0.2575          0.6638   \n",
            "4             0.2050           0.4000                0.1625          0.2364   \n",
            "\n",
            "   fractal_dimension_worst  Unnamed: 32  \n",
            "0                  0.11890          NaN  \n",
            "1                  0.08902          NaN  \n",
            "2                  0.08758          NaN  \n",
            "3                  0.17300          NaN  \n",
            "4                  0.07678          NaN  \n",
            "\n",
            "[5 rows x 33 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Cleaning\n",
        "\n",
        "# 1. Drop unused columns\n",
        "drop_columns = ['Unnamed: 32', 'id', 'diagnosis']\n",
        "\n",
        "# Convert Strings ('M' or 'B') -> Integers ('1' or '0')\n",
        "d = {'M': 0, 'B': 1} #for mapping Malignant('M') = 0 and Benign ('B') = 1\n",
        "\n",
        "# 2. Define features and labels\n",
        "y = data_raw['diagnosis'].map(d) #map 'M' as 0 and 'B' as 1 to each letter at the 'diagnosis' column\n",
        "X = data_raw.drop(drop_columns, axis=1) #drops 3 columns: 'Unnamed:33', 'ID' and 'diagnosis'\n",
        "print(X.head()) #head.() prints only the first 5 rows\n",
        "\n",
        "print(' ') #return a new space\n",
        "print('X.shape: ',X.shape) #[569 rows x 30 feature columns]\n",
        "print('y.shape: ',y.shape) #(569 rows, 1 column)"
      ],
      "metadata": {
        "id": "2FamYJIGoBtZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb59eb3-95fa-42da-8eb5-76fe447e5c18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
            "0                 0.07871  ...         25.38          17.33           184.60   \n",
            "1                 0.05667  ...         24.99          23.41           158.80   \n",
            "2                 0.05999  ...         23.57          25.53           152.50   \n",
            "3                 0.09744  ...         14.91          26.50            98.87   \n",
            "4                 0.05883  ...         22.54          16.67           152.20   \n",
            "\n",
            "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
            "0      2019.0            0.1622             0.6656           0.7119   \n",
            "1      1956.0            0.1238             0.1866           0.2416   \n",
            "2      1709.0            0.1444             0.4245           0.4504   \n",
            "3       567.7            0.2098             0.8663           0.6869   \n",
            "4      1575.0            0.1374             0.2050           0.4000   \n",
            "\n",
            "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
            "0                0.2654          0.4601                  0.11890  \n",
            "1                0.1860          0.2750                  0.08902  \n",
            "2                0.2430          0.3613                  0.08758  \n",
            "3                0.2575          0.6638                  0.17300  \n",
            "4                0.1625          0.2364                  0.07678  \n",
            "\n",
            "[5 rows x 30 columns]\n",
            " \n",
            "X.shape:  (569, 30)\n",
            "y.shape:  (569,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. testing: length of y.values\n",
        "print(y) #column of 'Diagnosis' is now a column of 569 '0's and '1's\n",
        "print(' ') #return a new space\n",
        "print('All', len(y.values),'Diagnosis printed out (y.values):')\n",
        "print(y.values) #print all 569 values"
      ],
      "metadata": {
        "id": "p0fov-seq7kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "shizgBEbkFvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New method to scale the input data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "encode_array = OneHotEncoder()\n",
        "y_2OP = encode_array.fit_transform(y[:, None]).toarray()\n",
        "#print(y_2OP.shape) #(569, 2)\n",
        "\n",
        "# Split the dataset into training (75%) and test (25%) \n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_2OP, test_size=0.25, random_state=0)"
      ],
      "metadata": {
        "id": "Lsxb1xNrGevc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN (another method)\n",
        "\n",
        "# Normalization of the X dataset (X_train and X_test), make it between 0-1, more fair and comparable between features\n",
        "X_train_n = (X_train-X_train.mean())/(X_train.max()-X_train.min()) # + and - values close to 1.0\n",
        "X_test_n = (X_test-X_train.mean())/(X_test.max()-X_test.min()) \n",
        "#print(X_train_n.shape)    #a matrix of [455 rows x 30 columns]\n",
        "#print(y_train.shape)      #a matrix of [455 rows x 1 column]\n",
        "#print('Diagnosis of the patients 0, 1, 2:')\n",
        "#print(y[0:3])              #testing: prints diagnosis of the patients 0, 1, 2 \n",
        "#print('X_train:', X_train)\n",
        "print('X_train.mean():', X_train.mean())\n",
        "print('X_train.max()',X_train.max())\n",
        "#print(X_train_n)\n",
        "print('y.shape:', y.shape)\n",
        "print('X_train:', X_train.shape)\n",
        "print('y_train:', y_train.shape)"
      ],
      "metadata": {
        "id": "UaWB9UdjkJL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score \n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score"
      ],
      "metadata": {
        "id": "GYGHWME9kLQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN model #1 and #2"
      ],
      "metadata": {
        "id": "2JpbiCaeCXSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define ANN model 1 structure\n",
        "# simple 1 hidden layer, 10 hidden nodes; 2 outputs with softmax\n",
        "# using all 30 features as input \n",
        "num_hidden_nodes = 50 #can change \n",
        "num_output = 2\n",
        "num_input = 30\n",
        "\n",
        "def model_1():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_hidden_nodes, input_dim= num_input, activation='relu')) #Dense : a regular fully connected layer\n",
        "    model.add(Dense(num_output, activation='softmax'))\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # Choice of optimizer: adam (adaptive moment estimation), AdaGrad (adaptive learning rate), \n",
        "    # sgd (Stochastic gradient descent), RMSprop (similar to AdaGrad), Adadelta (adaptive delta) ...\n",
        "    return model\n",
        "\n",
        "num_hidden_node1 = 40 #can change \n",
        "num_hidden_node2 = 40 #can change \n",
        "num_hidden_node3 = 40 #can change \n",
        "num_output = 2\n",
        "num_input = 30\n",
        "\n",
        "def model_2():\n",
        "    # create model with 2 hidden nodes\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_hidden_node1, input_dim= num_input, activation='relu')) #Dense : a regular fully connected layer\n",
        "    model.add(Dense(num_hidden_node2, activation='relu')) \n",
        "    model.add(Dense(num_hidden_node3, activation='relu')) \n",
        "    model.add(Dense(num_output, activation='softmax'))\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # Choice of optimizer: adam (adaptive moment estimation), AdaGrad (adaptive learning rate), \n",
        "    # sgd (Stochastic gradient descent), RMSprop (similar to AdaGrad), Adadelta (adaptive delta) ...\n",
        "    return model"
      ],
      "metadata": {
        "id": "kC8ynTaqkMjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the model\n",
        "model = model_2() #either model_1 or model_2\n",
        "# Training the ANN model 1\n",
        "history = model.fit(X_train, y_train, batch_size=8, epochs=60,verbose=2, validation_data=(X_test, y_test))\n",
        "# If difference in validation and training curve is too big, it indicates overfitting. \n",
        "\n",
        "# Testing of the trained ANN with X_test\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "CPRVQZmYt0XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting ANN model_1 or model_2: To show the performance of the ANN during training\n",
        "# The model information has already been saved in \"history\"\n",
        "\n",
        "metrics = history.history\n",
        "plt.plot(history.epoch, metrics['accuracy'], metrics['val_accuracy'])\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8aXgx_Sdvney"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.predict gives the ANN model 1 output with an input\n",
        "y_ANN_output = model.predict(X_test[0,None])\n",
        "print('y_ANN_output = ',y_ANN_output)"
      ],
      "metadata": {
        "id": "C6cl7XQdvuwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To check the softmax output from the ANN model 1\n",
        "check_sum = y_ANN_output[0]\n",
        "print('check_sum = ',check_sum[0]+check_sum[1]+check_sum[2])\n",
        "\n",
        "# To get the class label from Y_test\n",
        "y_ref_result = Y_test[0].argmax().item()\n",
        "print('y_ref_result = ',y_ref_result)\n",
        "\n",
        "# To get the class label from ANN\n",
        "y_ANN_result = y_ANN_output.argmax().item()\n",
        "print('y_ANN_result = ',y_ANN_result)"
      ],
      "metadata": {
        "id": "bslOUucdv0Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN Model #3 and #4\n",
        "\n",
        " 9 features:\n",
        "worst concave points ; concave points error;  concavity error\n",
        "\n",
        "Done: worst smoothness ; worst symmetry ; worst texture ; compactness error ; ; mean symmetry ; radius error "
      ],
      "metadata": {
        "id": "OEj9UvWIBGsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#select the 9 features from X\n",
        "\n",
        "print('X.shape: ',X.shape) #[569 rows x 30 feature columns]\n",
        "keep_9_features = ['id', 'diagnosis', 'radius_mean','texture_mean', \n",
        "                   'perimeter_mean', 'area_mean', 'smoothness_mean', \n",
        "                   'compactness_mean', 'concavity_mean', 'concave points_mean',\n",
        "                   'perimeter_worst', 'compactness_worst', 'concavity_worst',\n",
        "                   'fractal_dimension_worst','Unnamed: 32', 'area_worst', \n",
        "                   'fractal_dimension_mean','symmetry_se', 'radius_worst',\n",
        "                   'smoothness_se', 'area_se', 'texture_se', 'fractal_dimension_se',\n",
        "                   'perimeter_se'] #write down the 21 columns you want to drop\n",
        "\n",
        "XX = data_raw.drop(keep_9_features, axis=1) #drops 21 columns, keeps 9 features\n",
        "print(XX)\n",
        "print('XX.shape:', XX.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaQVXioqa787",
        "outputId": "76357758-2a3f-4ce9-93ed-1a9a94a13895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape:  (569, 14)\n",
            "     symmetry_mean  radius_se  compactness_se  concavity_se  \\\n",
            "0           0.2419     1.0950         0.04904       0.05373   \n",
            "1           0.1812     0.5435         0.01308       0.01860   \n",
            "2           0.2069     0.7456         0.04006       0.03832   \n",
            "3           0.2597     0.4956         0.07458       0.05661   \n",
            "4           0.1809     0.7572         0.02461       0.05688   \n",
            "..             ...        ...             ...           ...   \n",
            "564         0.1726     1.1760         0.02891       0.05198   \n",
            "565         0.1752     0.7655         0.02423       0.03950   \n",
            "566         0.1590     0.4564         0.03731       0.04730   \n",
            "567         0.2397     0.7260         0.06158       0.07117   \n",
            "568         0.1587     0.3857         0.00466       0.00000   \n",
            "\n",
            "     concave points_se  texture_worst  smoothness_worst  concave points_worst  \\\n",
            "0              0.01587          17.33           0.16220                0.2654   \n",
            "1              0.01340          23.41           0.12380                0.1860   \n",
            "2              0.02058          25.53           0.14440                0.2430   \n",
            "3              0.01867          26.50           0.20980                0.2575   \n",
            "4              0.01885          16.67           0.13740                0.1625   \n",
            "..                 ...            ...               ...                   ...   \n",
            "564            0.02454          26.40           0.14100                0.2216   \n",
            "565            0.01678          38.25           0.11660                0.1628   \n",
            "566            0.01557          34.12           0.11390                0.1418   \n",
            "567            0.01664          39.42           0.16500                0.2650   \n",
            "568            0.00000          30.37           0.08996                0.0000   \n",
            "\n",
            "     symmetry_worst  \n",
            "0            0.4601  \n",
            "1            0.2750  \n",
            "2            0.3613  \n",
            "3            0.6638  \n",
            "4            0.2364  \n",
            "..              ...  \n",
            "564          0.2060  \n",
            "565          0.2572  \n",
            "566          0.2218  \n",
            "567          0.4087  \n",
            "568          0.2871  \n",
            "\n",
            "[569 rows x 9 columns]\n",
            "XX.shape: (569, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New method to scale the input data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "XX_scaled = scaler.fit_transform(XX)\n",
        "\n",
        "encode_array = OneHotEncoder()\n",
        "y_2OP = encode_array.fit_transform(y[:, None]).toarray()\n",
        "#print(y_2OP.shape) #(569, 2)\n",
        "\n",
        "# Split the dataset into training (75%) and test (25%) \n",
        "XX_train, XX_test, y_train, y_test = train_test_split(XX_scaled, y_2OP, test_size=0.25, random_state=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIJbi1N5e-le",
        "outputId": "0c91e394-bc91-495e-c187-e821d80aded6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-c4c00282d709>:7: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  y_2OP = encode_array.fit_transform(y[:, None]).toarray()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Define ANN model 3 and model 4 structure\n",
        "# 1 hidden layer; 10 hidden nodes; 2 outputs with softmax\n",
        "# 2 outputs: 0 for 'malignant' is denoted as [0 1]; 1 for 'Benign' is denoted as [1 0]\n",
        "# using all 9 features\n",
        "\n",
        "num_hidden_nodes = 50 #can change \n",
        "num_output = 2\n",
        "num_input = 9 #features\n",
        "\n",
        "def model_3():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_hidden_nodes, input_dim= num_input, activation='relu')) #Dense : a regular fully connected layer\n",
        "    model.add(Dense(num_output, activation='softmax'))\n",
        "    # Compile model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # Choice of optimizer: adam (adaptive moment estimation), AdaGrad (adaptive learning rate), \n",
        "    # sgd (Stochastic gradient descent), RMSprop (similar to AdaGrad), Adadelta (adaptive delta) ...\n",
        "    return model\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "num_hidden_node1 = 40 #can change \n",
        "num_hidden_node2 = 40 #can change \n",
        "num_hidden_node3 = 40 #can change \n",
        "num_output = 2\n",
        "num_input = 9 #features\n",
        "\n",
        "def model_4():\n",
        "    # create model with 2 hidden nodes\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_hidden_node1, input_dim= num_input, activation='relu')) #Dense : a regular fully connected layer\n",
        "    model.add(Dense(num_hidden_node2, activation='relu')) \n",
        "    model.add(Dense(num_hidden_node3, activation='relu')) \n",
        "    model.add(Dense(num_output, activation='softmax'))\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # Choice of optimizer: adam (adaptive moment estimation), AdaGrad (adaptive learning rate), \n",
        "    # sgd (Stochastic gradient descent), RMSprop (similar to AdaGrad), Adadelta (adaptive delta) ...\n",
        "    return model"
      ],
      "metadata": {
        "id": "R3YStEhBA_8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the model_3 or model_4\n",
        "model = model_3() #either model_1 or model_2\n",
        "# Training the ANN model 1\n",
        "history = model.fit(XX_train, y_train, batch_size=8, epochs=60,verbose=2, validation_data=(XX_test, y_test))\n",
        "# If difference in validation and training curve is too big, it indicates overfitting. \n",
        "\n",
        "# Testing of the trained ANN with X_test\n",
        "score = model.evaluate(XX_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K170oPaQfSH0",
        "outputId": "755f72ea-e709-4cea-baee-a89c6580e3ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "54/54 - 1s - loss: 0.6408 - accuracy: 0.6362 - val_loss: 0.4576 - val_accuracy: 0.8182 - 864ms/epoch - 16ms/step\n",
            "Epoch 2/60\n",
            "54/54 - 0s - loss: 0.3729 - accuracy: 0.8897 - val_loss: 0.3169 - val_accuracy: 0.8741 - 118ms/epoch - 2ms/step\n",
            "Epoch 3/60\n",
            "54/54 - 0s - loss: 0.2643 - accuracy: 0.9202 - val_loss: 0.2554 - val_accuracy: 0.9021 - 164ms/epoch - 3ms/step\n",
            "Epoch 4/60\n",
            "54/54 - 0s - loss: 0.2082 - accuracy: 0.9390 - val_loss: 0.2171 - val_accuracy: 0.9231 - 134ms/epoch - 2ms/step\n",
            "Epoch 5/60\n",
            "54/54 - 0s - loss: 0.1741 - accuracy: 0.9531 - val_loss: 0.1939 - val_accuracy: 0.9441 - 138ms/epoch - 3ms/step\n",
            "Epoch 6/60\n",
            "54/54 - 0s - loss: 0.1517 - accuracy: 0.9531 - val_loss: 0.1771 - val_accuracy: 0.9580 - 122ms/epoch - 2ms/step\n",
            "Epoch 7/60\n",
            "54/54 - 0s - loss: 0.1357 - accuracy: 0.9554 - val_loss: 0.1661 - val_accuracy: 0.9580 - 118ms/epoch - 2ms/step\n",
            "Epoch 8/60\n",
            "54/54 - 0s - loss: 0.1231 - accuracy: 0.9577 - val_loss: 0.1579 - val_accuracy: 0.9580 - 124ms/epoch - 2ms/step\n",
            "Epoch 9/60\n",
            "54/54 - 0s - loss: 0.1136 - accuracy: 0.9601 - val_loss: 0.1504 - val_accuracy: 0.9580 - 152ms/epoch - 3ms/step\n",
            "Epoch 10/60\n",
            "54/54 - 0s - loss: 0.1059 - accuracy: 0.9648 - val_loss: 0.1465 - val_accuracy: 0.9580 - 154ms/epoch - 3ms/step\n",
            "Epoch 11/60\n",
            "54/54 - 0s - loss: 0.0997 - accuracy: 0.9648 - val_loss: 0.1422 - val_accuracy: 0.9580 - 138ms/epoch - 3ms/step\n",
            "Epoch 12/60\n",
            "54/54 - 0s - loss: 0.0949 - accuracy: 0.9648 - val_loss: 0.1405 - val_accuracy: 0.9580 - 159ms/epoch - 3ms/step\n",
            "Epoch 13/60\n",
            "54/54 - 0s - loss: 0.0904 - accuracy: 0.9671 - val_loss: 0.1374 - val_accuracy: 0.9580 - 154ms/epoch - 3ms/step\n",
            "Epoch 14/60\n",
            "54/54 - 0s - loss: 0.0868 - accuracy: 0.9624 - val_loss: 0.1387 - val_accuracy: 0.9580 - 169ms/epoch - 3ms/step\n",
            "Epoch 15/60\n",
            "54/54 - 0s - loss: 0.0831 - accuracy: 0.9671 - val_loss: 0.1350 - val_accuracy: 0.9580 - 158ms/epoch - 3ms/step\n",
            "Epoch 16/60\n",
            "54/54 - 0s - loss: 0.0802 - accuracy: 0.9671 - val_loss: 0.1354 - val_accuracy: 0.9580 - 161ms/epoch - 3ms/step\n",
            "Epoch 17/60\n",
            "54/54 - 0s - loss: 0.0779 - accuracy: 0.9718 - val_loss: 0.1344 - val_accuracy: 0.9580 - 126ms/epoch - 2ms/step\n",
            "Epoch 18/60\n",
            "54/54 - 0s - loss: 0.0755 - accuracy: 0.9718 - val_loss: 0.1338 - val_accuracy: 0.9580 - 112ms/epoch - 2ms/step\n",
            "Epoch 19/60\n",
            "54/54 - 0s - loss: 0.0731 - accuracy: 0.9718 - val_loss: 0.1344 - val_accuracy: 0.9510 - 125ms/epoch - 2ms/step\n",
            "Epoch 20/60\n",
            "54/54 - 0s - loss: 0.0713 - accuracy: 0.9718 - val_loss: 0.1342 - val_accuracy: 0.9580 - 119ms/epoch - 2ms/step\n",
            "Epoch 21/60\n",
            "54/54 - 0s - loss: 0.0696 - accuracy: 0.9718 - val_loss: 0.1336 - val_accuracy: 0.9580 - 157ms/epoch - 3ms/step\n",
            "Epoch 22/60\n",
            "54/54 - 0s - loss: 0.0681 - accuracy: 0.9718 - val_loss: 0.1329 - val_accuracy: 0.9441 - 120ms/epoch - 2ms/step\n",
            "Epoch 23/60\n",
            "54/54 - 0s - loss: 0.0665 - accuracy: 0.9718 - val_loss: 0.1338 - val_accuracy: 0.9441 - 116ms/epoch - 2ms/step\n",
            "Epoch 24/60\n",
            "54/54 - 0s - loss: 0.0650 - accuracy: 0.9718 - val_loss: 0.1356 - val_accuracy: 0.9441 - 164ms/epoch - 3ms/step\n",
            "Epoch 25/60\n",
            "54/54 - 0s - loss: 0.0643 - accuracy: 0.9742 - val_loss: 0.1348 - val_accuracy: 0.9441 - 119ms/epoch - 2ms/step\n",
            "Epoch 26/60\n",
            "54/54 - 0s - loss: 0.0630 - accuracy: 0.9742 - val_loss: 0.1340 - val_accuracy: 0.9441 - 162ms/epoch - 3ms/step\n",
            "Epoch 27/60\n",
            "54/54 - 0s - loss: 0.0617 - accuracy: 0.9718 - val_loss: 0.1367 - val_accuracy: 0.9441 - 154ms/epoch - 3ms/step\n",
            "Epoch 28/60\n",
            "54/54 - 0s - loss: 0.0615 - accuracy: 0.9742 - val_loss: 0.1366 - val_accuracy: 0.9441 - 155ms/epoch - 3ms/step\n",
            "Epoch 29/60\n",
            "54/54 - 0s - loss: 0.0600 - accuracy: 0.9742 - val_loss: 0.1358 - val_accuracy: 0.9441 - 120ms/epoch - 2ms/step\n",
            "Epoch 30/60\n",
            "54/54 - 0s - loss: 0.0592 - accuracy: 0.9789 - val_loss: 0.1395 - val_accuracy: 0.9441 - 159ms/epoch - 3ms/step\n",
            "Epoch 31/60\n",
            "54/54 - 0s - loss: 0.0582 - accuracy: 0.9765 - val_loss: 0.1409 - val_accuracy: 0.9441 - 117ms/epoch - 2ms/step\n",
            "Epoch 32/60\n",
            "54/54 - 0s - loss: 0.0575 - accuracy: 0.9765 - val_loss: 0.1396 - val_accuracy: 0.9441 - 154ms/epoch - 3ms/step\n",
            "Epoch 33/60\n",
            "54/54 - 0s - loss: 0.0564 - accuracy: 0.9789 - val_loss: 0.1409 - val_accuracy: 0.9441 - 133ms/epoch - 2ms/step\n",
            "Epoch 34/60\n",
            "54/54 - 0s - loss: 0.0550 - accuracy: 0.9789 - val_loss: 0.1414 - val_accuracy: 0.9441 - 171ms/epoch - 3ms/step\n",
            "Epoch 35/60\n",
            "54/54 - 0s - loss: 0.0558 - accuracy: 0.9789 - val_loss: 0.1445 - val_accuracy: 0.9441 - 162ms/epoch - 3ms/step\n",
            "Epoch 36/60\n",
            "54/54 - 0s - loss: 0.0550 - accuracy: 0.9789 - val_loss: 0.1456 - val_accuracy: 0.9441 - 159ms/epoch - 3ms/step\n",
            "Epoch 37/60\n",
            "54/54 - 0s - loss: 0.0529 - accuracy: 0.9812 - val_loss: 0.1445 - val_accuracy: 0.9441 - 157ms/epoch - 3ms/step\n",
            "Epoch 38/60\n",
            "54/54 - 0s - loss: 0.0518 - accuracy: 0.9812 - val_loss: 0.1450 - val_accuracy: 0.9510 - 124ms/epoch - 2ms/step\n",
            "Epoch 39/60\n",
            "54/54 - 0s - loss: 0.0513 - accuracy: 0.9789 - val_loss: 0.1448 - val_accuracy: 0.9510 - 112ms/epoch - 2ms/step\n",
            "Epoch 40/60\n",
            "54/54 - 0s - loss: 0.0509 - accuracy: 0.9812 - val_loss: 0.1471 - val_accuracy: 0.9510 - 164ms/epoch - 3ms/step\n",
            "Epoch 41/60\n",
            "54/54 - 0s - loss: 0.0500 - accuracy: 0.9789 - val_loss: 0.1465 - val_accuracy: 0.9510 - 114ms/epoch - 2ms/step\n",
            "Epoch 42/60\n",
            "54/54 - 0s - loss: 0.0492 - accuracy: 0.9812 - val_loss: 0.1481 - val_accuracy: 0.9510 - 190ms/epoch - 4ms/step\n",
            "Epoch 43/60\n",
            "54/54 - 0s - loss: 0.0486 - accuracy: 0.9789 - val_loss: 0.1470 - val_accuracy: 0.9510 - 180ms/epoch - 3ms/step\n",
            "Epoch 44/60\n",
            "54/54 - 0s - loss: 0.0484 - accuracy: 0.9812 - val_loss: 0.1507 - val_accuracy: 0.9441 - 169ms/epoch - 3ms/step\n",
            "Epoch 45/60\n",
            "54/54 - 0s - loss: 0.0474 - accuracy: 0.9812 - val_loss: 0.1513 - val_accuracy: 0.9441 - 165ms/epoch - 3ms/step\n",
            "Epoch 46/60\n",
            "54/54 - 0s - loss: 0.0469 - accuracy: 0.9836 - val_loss: 0.1520 - val_accuracy: 0.9441 - 208ms/epoch - 4ms/step\n",
            "Epoch 47/60\n",
            "54/54 - 0s - loss: 0.0464 - accuracy: 0.9836 - val_loss: 0.1536 - val_accuracy: 0.9441 - 196ms/epoch - 4ms/step\n",
            "Epoch 48/60\n",
            "54/54 - 0s - loss: 0.0458 - accuracy: 0.9836 - val_loss: 0.1522 - val_accuracy: 0.9441 - 170ms/epoch - 3ms/step\n",
            "Epoch 49/60\n",
            "54/54 - 0s - loss: 0.0453 - accuracy: 0.9836 - val_loss: 0.1537 - val_accuracy: 0.9441 - 158ms/epoch - 3ms/step\n",
            "Epoch 50/60\n",
            "54/54 - 0s - loss: 0.0451 - accuracy: 0.9836 - val_loss: 0.1540 - val_accuracy: 0.9441 - 156ms/epoch - 3ms/step\n",
            "Epoch 51/60\n",
            "54/54 - 0s - loss: 0.0441 - accuracy: 0.9836 - val_loss: 0.1557 - val_accuracy: 0.9441 - 202ms/epoch - 4ms/step\n",
            "Epoch 52/60\n",
            "54/54 - 0s - loss: 0.0446 - accuracy: 0.9883 - val_loss: 0.1563 - val_accuracy: 0.9441 - 181ms/epoch - 3ms/step\n",
            "Epoch 53/60\n",
            "54/54 - 0s - loss: 0.0440 - accuracy: 0.9836 - val_loss: 0.1591 - val_accuracy: 0.9510 - 159ms/epoch - 3ms/step\n",
            "Epoch 54/60\n",
            "54/54 - 0s - loss: 0.0441 - accuracy: 0.9859 - val_loss: 0.1653 - val_accuracy: 0.9441 - 154ms/epoch - 3ms/step\n",
            "Epoch 55/60\n",
            "54/54 - 0s - loss: 0.0419 - accuracy: 0.9859 - val_loss: 0.1619 - val_accuracy: 0.9510 - 156ms/epoch - 3ms/step\n",
            "Epoch 56/60\n",
            "54/54 - 0s - loss: 0.0422 - accuracy: 0.9883 - val_loss: 0.1615 - val_accuracy: 0.9510 - 167ms/epoch - 3ms/step\n",
            "Epoch 57/60\n",
            "54/54 - 0s - loss: 0.0410 - accuracy: 0.9859 - val_loss: 0.1632 - val_accuracy: 0.9510 - 158ms/epoch - 3ms/step\n",
            "Epoch 58/60\n",
            "54/54 - 0s - loss: 0.0402 - accuracy: 0.9859 - val_loss: 0.1657 - val_accuracy: 0.9510 - 173ms/epoch - 3ms/step\n",
            "Epoch 59/60\n",
            "54/54 - 0s - loss: 0.0401 - accuracy: 0.9859 - val_loss: 0.1649 - val_accuracy: 0.9510 - 168ms/epoch - 3ms/step\n",
            "Epoch 60/60\n",
            "54/54 - 0s - loss: 0.0395 - accuracy: 0.9906 - val_loss: 0.1654 - val_accuracy: 0.9510 - 168ms/epoch - 3ms/step\n",
            "Test loss: 0.16542759537696838\n",
            "Test accuracy: 0.9510489702224731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze test results from model #1 and #2 using confusion matrix, recall score and f1_score."
      ],
      "metadata": {
        "id": "7eJruIHlSM-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion matrix \n",
        "actual = np.random.binomial(1, 0.9, size = 1000)\n",
        "predicted = np.random.binomial(1, 0.9, size = 1000)\n",
        "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
        "cm_display.plot()\n",
        "plt.show()\n",
        "\n",
        "#accuracy\n",
        "Accuracy = metrics.accuracy_score(actual, predicted)\n",
        "\n",
        "#precision\n",
        "Precision = metrics.precision_score(actual, predicted)\n",
        "\n",
        "#recall score\n",
        "\n",
        "ANN1_recall = metrics.recall_score(actual, predicted)\n",
        "\n",
        "\n",
        "#F1-score is the harmonic mean of the precision and recall\n",
        "ANN1_F1 = 2/((1/ANN1_precision)+(1/ANN1_recall))\n",
        "#F1-score = 2 * ((Precision * Sensitivity) / (Precision + Sensitivity))\n",
        "\n"
      ],
      "metadata": {
        "id": "qmFVO2moO_fH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "60aada0a-ce9e-49a8-a749-65b16d55042f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-75170ee365fc>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcm_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcm_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_8Fc8gnAYsQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To show the performance of the ANN during training\n",
        "\n",
        "metrics = history.history\n",
        "plt.plot(history.epoch, metrics['accuracy'], metrics['val_accuracy'])\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1spRLoQCYsj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze test results of Model #3 and Model #4 using confusion matric, recall score and f1_score."
      ],
      "metadata": {
        "id": "tU1yAkAOWCH4"
      }
    }
  ]
}